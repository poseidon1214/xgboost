#!/usr/bin/python
import sys
import numpy as np
import scipy.sparse
# append the path to xgboost, you may need to change the following line
sys.path.append('./')
import xgboost as xgb

### simple example
# load file from text file, also binary buffer generated by xgboost
#dtrain = xgb.DMatrix('train_fea')
dtrain = xgb.DMatrix('train_fea')
dtest = xgb.DMatrix('test_fea')

# specify parameters via map, definition are same as c++ version
#param = {'bst:max_depth':2, 'bst:eta':1, 'silent':1, 'objective':'binary:logistic' }
param = {'booster_type':0,
         'bst:max_depth':2, 
         'bst:eta':1, 
         'bst:min_child_weight':3,
         'silent':1, 
         'objective':'binary:logistic',
         'eval_metric':'auc'
         }

# specify validations set to watch performance
evallist  = [(dtest,'eval'), (dtrain,'train')]
num_round = 1000
bst = xgb.train( param, dtrain, num_round, evallist )

# this is prediction
preds = bst.predict( dtest )
labels = dtest.get_label()
#print ('error=%f' % (  sum(1 for i in range(len(preds)) if int(preds[i]>0.5)!=labels[i]) /float(len(preds))))
for i in range(len(preds)):
    sys.stdout.write('%f\t%d\n' %(preds[i], labels[i]))
bst.save_model('0001.model')
# dump model
#bst.dump_model('dump.raw.txt')
# dump model with feature map
bst.dump_model('dump.raw.txt','featmap.txt')
'''
###
# build dmatrix in python iteratively
#
print ('start running example of build DMatrix in python')
dtrain = xgb.DMatrix()
labels = []
for l in open('agaricus.txt.train'):
    arr = l.split()
    labels.append( int(arr[0]))
    feats = []
    for it in arr[1:]:
        k,v = it.split(':')
        feats.append( (int(k), float(v)) )
    dtrain.add_row( feats )
dtrain.set_label( labels )
evallist  = [(dtest,'eval'), (dtrain,'train')]

bst = xgb.train( param, dtrain, num_round, evallist )

###
# build dmatrix from scipy.sparse
print ('start running example of build DMatrix from scipy.sparse')
labels = []
row = []; col = []; dat = []
i = 0
for l in open('agaricus.txt.train'):
    arr = l.split()
    labels.append( int(arr[0]))
    for it in arr[1:]:
        k,v = it.split(':')
        row.append(i); col.append(int(k)); dat.append(float(v))
    i += 1

csr = scipy.sparse.csr_matrix( (dat, (row,col)) )
dtrain = xgb.DMatrix( csr )
dtrain.set_label(labels)
evallist  = [(dtest,'eval'), (dtrain,'train')]
bst = xgb.train( param, dtrain, num_round, evallist )

print ('start running example of build DMatrix from numpy array')
# NOTE: npymat is numpy array, we will convert it into scipy.sparse.csr_matrix in internal implementation,then convert to DMatrix
npymat = csr.todense()
dtrain = xgb.DMatrix( npymat )
dtrain.set_label(labels)
evallist  = [(dtest,'eval'), (dtrain,'train')]
bst = xgb.train( param, dtrain, num_round, evallist )

###
# advanced: cutomsized loss function, set loss_type to 0, so that predict get untransformed score
# 
print ('start running example to used cutomized objective function')

# note: set objective= binary:logistic means the prediction will get logistic transformed
#       in most case, we may want to leave it as default
param = {'bst:max_depth':2, 'bst:eta':1, 'silent':1, 'objective':'binary:logistic' }

# user define objective function, given prediction, return gradient and second order gradient
def logregobj( preds, dtrain ):
    labels = dtrain.get_label()
    grad = preds - labels
    hess = preds * (1.0-preds)
    return grad, hess

# training with customized objective, we can also do step by step training, simply look at xgboost.py's implementation of train
bst = xgb.train( param, dtrain, num_round, evallist, logregobj )
'''
